{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path.cwd().parent / 'data'\n",
    "clean_data_path = DATA_DIR / 'processed' / 'ames_clean.pkl'\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening file with cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_path = DATA_DIR / 'processed' / 'ames_clean.pkl'\n",
    "\n",
    "with open(clean_data_path, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite some outliers were already removed on \"02_analysis_and_preprocessing.ipynb\", by reading the documentation we found this piece of information:\n",
    "\n",
    "```There are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them quickly). Three of them are true outliers (Partial Sales that likely donï¿½t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations) before assigning it to students.```\n",
    "\n",
    "So let's check if Prof. Ayres has already removed this outliers highlitghed in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Gr.Liv.Area'], data.SalePrice, 'o', alpha=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, he did not. So let's remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Gr.Liv.Area'] < 4000]\n",
    "\n",
    "plt.plot(data['Gr.Liv.Area'], data.SalePrice, 'o', alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's better! Now let's start the data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of possible data transformations to improve model performance. To understand which ones make sense to AMES dataset, it is necessary to investigate and understand data analysis made in notebook \"02_analysis_and_processing.ipynb\". One characteristic that stood out from some features was the concentrations to the left in the scatter plots. It may mean that calculating the log of the value can improve correlation with target variable. To check if this is true, it is necessary to select only numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_variables = [\n",
    "    'Lot.Frontage',\n",
    "    'Lot.Area',\n",
    "    'Mas.Vnr.Area',\n",
    "    'BsmtFin.SF.1',\n",
    "    'BsmtFin.SF.2',\n",
    "    'Bsmt.Unf.SF',\n",
    "    'Total.Bsmt.SF',\n",
    "    'X1st.Flr.SF',\n",
    "    'X2nd.Flr.SF',\n",
    "    'Low.Qual.Fin.SF',\n",
    "    'Gr.Liv.Area',\n",
    "    'Garage.Area',\n",
    "    'Wood.Deck.SF',\n",
    "    'Open.Porch.SF',\n",
    "    'Enclosed.Porch',\n",
    "    'X3Ssn.Porch',\n",
    "    'Screen.Porch',\n",
    "    'Pool.Area',\n",
    "    'Misc.Val',\n",
    "]\n",
    "\n",
    "continuous_data = data[continuous_variables].copy()\n",
    "continuous_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can validate our idea. One way to understand if makes sense calculating log values is checking data distribution. It was checked on notebook \"02.1_some_more_analysis.ipynb\" and it was confirmed that some features have a distribution that can be improved by calculating log values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go check charts again and come back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in continuous_variables:\n",
    "\n",
    "    num_nonzero_data = continuous_data[continuous_data[col] != 0]\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    sns.distplot(num_nonzero_data[col], ax=ax1)\n",
    "    stats.probplot(num_nonzero_data[col], plot=ax2)\n",
    "    stats.probplot(np.log(num_nonzero_data[col]), plot=ax3)\n",
    "\n",
    "    ax1.set_title(col)\n",
    "    ax2.set_title('probplot')\n",
    "    ax3.set_title('probplot log')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. It seems that our idea was right. We can calculate log values for the features that, by calculating, increase similarity to normal distribution. Other strategy is to calculate log values for all numerical features and check if it improves correlation with target data. Let's do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking log correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, series in continuous_data.items():\n",
    "    # Calculate correlation between the two columns\n",
    "    corr = series.corr(target)\n",
    "\n",
    "    series = series.loc[series != 0]\n",
    "    log_series = series.apply(np.log)\n",
    "    corr_log = log_series.corr(target)\n",
    "    \n",
    "    if abs(corr_log) > (abs(corr) + 0.05):\n",
    "        print(\"Correlation between\", column, \"and the target is\", corr)\n",
    "        print(\"Correlation between log(\"+column+\") and the target is\", corr_log)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yyyyaaaaaayyy! Correlation increases on \"Lot.Area\", \"BsmtFin.SF.2\", \"X2nd.Flr.SF\", \"Low.Qual.Fin.SF\", \"Enclosed.Porch\", \"X3Ssn.Porch\", \"Screen.Porch\", \"Pool.Area\" and \"Misc.Val\" when log is calculated. Let's create a list with features with better correlation and improvement in distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_log = ('Gr.Liv.Area', \n",
    "                  'Lot.Area', \n",
    "                  'BsmtFin.SF.2', \n",
    "                  'X2nd.Flr.SF',\n",
    "                  'Low.Qual.Fin.SF', \n",
    "                  'Enclosed.Porch', \n",
    "                  'X3Ssn.Porch', \n",
    "                  'Screen.Porch',\n",
    "                  'Pool.Area', \n",
    "                  'Misc.Val', \n",
    "                  'Open.Porch.SF', \n",
    "                  'Wood.Deck.SF', \n",
    "                  'Garage.Area',\n",
    "                  'X1st.Flr.SF', \n",
    "                  'Total.Bsmt.SF', \n",
    "                  'Bsmt.Unf.SF', \n",
    "                  'Mas.Vnr.Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great strategy to improve performance on models is to scale the numerical data. This strategy has no effect on models like Decisions Trees, but have a huge impact on linear models like Elastic Net.\n",
    "\n",
    "Checking boxplot charts, lots of outliers were noticed. If data is scaled by minimum and maximun values, data quality will be impacted because of the effect of outliers. Instead of applying min and max scaling, a better strategy is to apply standard scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = data.select_dtypes(include=['float64'])\n",
    "numerical_columns = []\n",
    "\n",
    "for column in numerical_data.columns:\n",
    "    numerical_columns.append(column)\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    (\"log_calculation\", FunctionTransformer(np.log, validate=True), columns_to_log),\n",
    "    (\"scaler\", StandardScaler(), numerical_columns),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = transformer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_path = DATA_DIR / 'processed' / 'ames_transformed.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(transformed_data_path, 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a regression model for predicting house sale prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path.cwd().parent / 'data'\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_path = DATA_DIR / 'processed' / 'ames_transformed.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(clean_data_path, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data all cleaned, and all the missing values accounted for, lets focus on transforming the data for the model.\n",
    "\n",
    "Lets remember what a model is. \n",
    "\n",
    "- A predictive model is a **set** of functions that receive data as input and produce a prediction, that is, an estimate of the target value as output.\n",
    "- **Train** a model is to search the set of candidate functions for one that adequately represents the **training dataset**.\n",
    "- The adequacy of a candidate function to the training data is usually determined by a **loss function** that measures how well the predictions of the function match the real values of the target within the training dataset. It is common to define a *loss function per data item* (e.g. absolute error, quadratic error, etc) and to construct the *loss function over the dataset* as the *average prediction loss*.\n",
    "\n",
    "Many models are **parametric models**. In this case, each function of the set of functions that makes the model is constructed from a vector of **parameters** that define the function, forming a **parametric function**. For instance: the linear model constructs prediction values out of a linear combination of the input features, plus a constant. The weights of the linear combination plus the constant are the parameters of the model. The set of functions that can be represented by this model is given by all possible values of the vector of parameters that define the function.\n",
    "\n",
    "Some models are called **non-parametric models**. These models usually do not have a parametric form (like the linear model). But the terminology is a bit misleading, though: usually these models *do* have parameters, and potentially an open-ended set of them! For instance, consider the \"decision tree\" model, which is one of the most prominent models of this category. The decision tree may not have a formula for the predicted value, but it does have parameters, many of them: each decision in the tree involves a choice of feature and a threshold level, and those choices must be stored as parameters of the model for use in future predictions.\n",
    "\n",
    "Each model has specific requirements for the format of the input data. Most of the time, the minimum requirement is that:\n",
    "\n",
    "- All columns are numeric;\n",
    "- There are no missing values.\n",
    "\n",
    "Some models have extra requirements. For example: the support-vector-machines model requires that the input features have comparable standard deviations - having features that have large discrepancies between features in terms of their order of magnitude (such as a feature in the fractions of unit range and another in the tens of thousands) will result in poor prediction quality.\n",
    "\n",
    "And some models may not have any special requirement at all. We will study each of those in detail in this course.\n",
    "\n",
    "Lets start our study with a simple model: the *multivariate linear regression* model. This is a model that presents the minimum requirements listed above. So we need to do a bit of processing on the original features:\n",
    "\n",
    "- *Numerical features* stay as given;\n",
    "- *Categorical features* have to be transformed into numerical features. In order to do so we need to **encode** these features, that is: to transform them into new features that convey the same information, but in a numerical form, and in a way that \"makes sense\" - we'll see it below.\n",
    "- *Ordinal features* can be transformed into numerical features in the same way as the caegorical features, or could be assigned increasing numbers in conformity with the ordered nature of the categories of the feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets identify all categorical variables - both nominal (that is, categoricals without category order) and ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "ordinal_columns = []\n",
    "for col in model_data.select_dtypes('category').columns:\n",
    "    if model_data[col].cat.ordered:\n",
    "        ordinal_columns.append(col)\n",
    "    else:\n",
    "        categorical_columns.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding ordinal variables "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal variables can be transformed into integer numbers in a straightforward manner: the lowest category is assigned the value \"zero\", the next category is given the value \"one\", etc. The `Pandas` library has a function for this task: `factorize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ordinal_columns:\n",
    "    codes, _ = pd.factorize(data[col], sort=True)\n",
    "    model_data[col] = codes / codes.max() ## Acling on the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets confirm that the variables are no longer ordinal, but now are integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[ordinal_columns].info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the original values with the encoded values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Lot.Shape'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['Lot.Shape'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[ordinal_columns].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding nominal variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With nominal variables there is no notion of order among categories. Therefore, it would be a conceptual mistake to encode them in the same manner as the ordinal variables. For instance, consider the `Exterior` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['Exterior'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot assign an order here, lest we end up with equations like `HdBoard` + `Plywood` = `CemntBd`, which are nonsense. \n",
    "\n",
    "The strategy here to encode `Exterior` is to create several new numerical variables to represent the membership of a given data item to one of the `Exterior` categories. These are called **dummy variables**. Each of these new variables contain only the values \"zero\" or \"one\" (i.e. they are binary variables), where $1$ denotes that the data item belongs to the category represented by the variable. Evidently, for a given data item, only one dummy variable has a value of $1$, all remaining are $0$.\n",
    "\n",
    "There are two types of dummy variable encoding:\n",
    "\n",
    "- \"One-hot\" encoding: in this case we create one dummy variable per category. Let's look at the `Exterior` feature as an example. The `Pandas` function `get_dummies()` can do the encoding for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = model_data['Exterior']\n",
    "encoded_data = pd.get_dummies(original_data)\n",
    "\n",
    "aux_dataframe = encoded_data\n",
    "aux_dataframe['Exterior'] = original_data.copy()\n",
    "\n",
    "aux_dataframe.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that for each value of `Exterior`, only the corresponding dummy is flagged.\n",
    "\n",
    "One-hot encoding is a popular technique in Machine Learning. Statisticians, however, prefer a slightly different way of dummy encoding which is:\n",
    "\n",
    "- Choose a category to *not encode* (this is called the *base category*)\n",
    "- Generate dummies for the remaining categories. That is:\n",
    "    - If the data item belongs to the base category, no dummy receives a value of $1$;\n",
    "    - Otherwise, set the corresponding dummy to $1$.\n",
    "\n",
    "The same `get_dummies()` function of `Pandas` can do this automatically with the `drop_first` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = model_data['Exterior']\n",
    "encoded_data = pd.get_dummies(original_data, drop_first=True)\n",
    "\n",
    "aux_dataframe = encoded_data\n",
    "aux_dataframe['Exterior'] = original_data.copy()\n",
    "\n",
    "aux_dataframe.head().transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are now missing the dummy variable for the `AsbShng` category.\n",
    "\n",
    "Why to encode things this way? If we don't drop one of the dummies, then it will always be the case that the sum of the values of the dummies is $1$ (since each data item must belong to one of the categories). The linear model, particularly very popular with the statisticians, implies the existence of a fictitious feature containing, for all data items, the value $1$. Hence we end up having a set of variables where a linear combination of them (in this case, the sum of the dummies) matches the value at another variable. This has numerical computing implications for the linear model, that we will discuss in class.\n",
    "\n",
    "Since we want to use the linear model in this notebook, lets encode all categoricals with the `drop_first` alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(model_data, drop_first=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset has a lot more variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categorical_columns:\n",
    "    dummies = []\n",
    "    for col in model_data.columns:\n",
    "        if col.startswith(cat + \"_\"):\n",
    "            dummies.append(f'\"{col}\"')\n",
    "    dummies_str = ', '.join(dummies)\n",
    "    print(f'From column \"{cat}\" we made {dummies_str}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test splitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will now be organized as follows:\n",
    "\n",
    "- The features form a matrix $X$ of size $m \\times n$, where $m$ is the number of data items, and $n$ is the number of features.\n",
    "- The target forms a column-matrix $y$ of length $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data.drop(columns=['SalePrice']).copy().values\n",
    "y = model_data['SalePrice'].copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the typical set-up of a machine learning project. Now we want to train our model *and* verify that the model provides good predictions for *unseen* data items. Why the emphasis on \"unseen\"? Because there is no use for a model that only gives predictions for the items in the data used to train it - we want our models to *generalize*.\n",
    "\n",
    "The way to assess the model's performance for unseen values is to split the dataset into two subsets: the **training** and **test** datasets.\n",
    "\n",
    "We have been using a lot of `Pandas` to manipulate our data so far. From now on we will switch to another very popular library for machine learning in Python: `Scikit-Learn`.\n",
    "\n",
    "The function `train_test_split()` will take as arguments the dataset to be split, the specification of the fraction of the dataset to be reserved for testing, and a random seed value - so that the split will always be the same whenever we run our notebook. This is a customary measure to ensure reproducibility of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42  # Any number here, really."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting some models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of different models to choose from. In this notebook we are going to fit a few of them and compare their performance. We will start with our baseline model, the linear regressor, then try Decision Tree, Random Forest and Elastic Net. We will also try to improve those models by tuning their hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class DummyModel(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def init(self):\n",
    "        self.mean = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.mean = y.mean()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.mean[0] * np.ones(shape=[X.shape[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaselineModel = LinearRegression()\n",
    "#DummyModel = DummyModel()\n",
    "RandomForestModel = RandomForestRegressor()\n",
    "DecisionTreeModel = DecisionTreeRegressor()\n",
    "ElasticNetModel = ElasticNet()\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Train and evaluate baseline model\n",
    "baselineModel.fit(Xtrain, ytrain)\n",
    "baselineScore = -cross_val_score(baselineModel, Xtrain, ytrain, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "results['Baseline'] = {\n",
    "    'best_params': None,\n",
    "    'best_score': baselineScore.mean(),\n",
    "    'best_model' : baselineModel\n",
    "}\n",
    "\n",
    "results['Baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_RandomForest = {\n",
    "    'n_estimators': [50, 100, 200],  \n",
    "    'max_depth': [None, 5, 10], \n",
    "    'min_samples_split': [1, 2, 5], \n",
    "    'min_samples_leaf': [1, 2, 4], \n",
    "    'bootstrap': [True, False],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "params_ElasticNet = {\n",
    "    'alpha': np.arange(0, 0.01, 0.001),  \n",
    "    'l1_ratio': np.arange(0, 0.01, 0.001)  \n",
    "}\n",
    "\n",
    "params_DecisionTree = {\n",
    "    'max_depth': np.arange(8, 12, 1),  \n",
    "    'min_samples_split': np.arange(0, 5, 1),  \n",
    "    'min_samples_leaf': np.arange(8, 12, 1), \n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "paramsList = [params_ElasticNet,\n",
    "              params_DecisionTree,\n",
    "              params_RandomForest]\n",
    "\n",
    "modelsList = [ElasticNetModel,\n",
    "              DecisionTreeModel,\n",
    "              RandomForestModel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "for model, params in zip(modelsList,paramsList):\n",
    "    \n",
    "    # grid search for each model and parameters\n",
    "   \n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        params,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    grid_search.fit(Xtrain, ytrain)\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    # Store the results\n",
    "    results[type(model).__name__] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_model' : grid_search.best_estimator_\n",
    "    }\n",
    "    \n",
    "    print(results[type(model).__name__]['best_params'])\n",
    "    print(f\"Finished {type(model).__name__} in {t2-t1:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the results based on mean test score\n",
    "sorted_results = sorted(results.items(), key=lambda x: -x[1]['best_score'])\n",
    "\n",
    "params = {}\n",
    "\n",
    "for model_name, result in sorted_results:\n",
    "    params[model_name] = result['best_params']\n",
    "\n",
    "    if model_name != 'Baseline':\n",
    "\n",
    "        print(f\"Results for {model_name}:\")\n",
    "        print(f\"Best Parameters: {result['best_params']}\")\n",
    "        print(f\"Best Score RMSE: {np.sqrt(-result['best_score'])}\")\n",
    "        print(f\"Mean Test Score %: {100*((10**(np.sqrt(-result['best_score'])))-1)}\")\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "    else:\n",
    "        print(f\"Results for {model_name}:\")\n",
    "        print(f\"Mean Test Score %: {100*((10**(np.sqrt(result['best_score'])))-1)}\")\n",
    "        print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our best model so far is the Elastic Net Regressor. Maybe we can discover more about the data by looking at the feature importances of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Feature importances of the best model so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "best_model = sorted_results[1][1]['best_model']\n",
    "\n",
    "# Get best model scores\n",
    "print(f\"Best model scores: {sorted_results[1][1]['best_score']}\")\n",
    "best_model_scores = sorted_results[1][1]['best_score']\n",
    "\n",
    "# Get feature importances from best model\n",
    "importances = best_model.feature_importances_\n",
    "\n",
    "# Get feature names from Xtrain\n",
    "feature_names = model_data.drop(columns=['SalePrice']).copy().columns\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "indices = importances.argsort()[::-1]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(40, 15))\n",
    "plt.title(\"Feature Importances\", fontsize=24)  # Increase title fontsize\n",
    "bars = plt.bar(range(Xtrain.shape[1]), importances[indices], align=\"center\")\n",
    "\n",
    "# Change color of bars with absolute values greater than 0.6\n",
    "for i, bar in enumerate(bars):\n",
    "    if abs(importances[indices][i]) > 0.05:\n",
    "        bar.set_color('green')\n",
    "\n",
    "plt.xticks(range(Xtrain.shape[1]), feature_names[indices], rotation=90, fontsize=16)  # Increase x-axis fontsize\n",
    "plt.yticks(fontsize=16)  # Increase y-axis fontsize\n",
    "plt.xlim([-1, Xtrain.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about combining models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods combine the predictions of multiple machine learning models to improve overall predictive performance. In the case of StackingRegressor, it combines the outputs of multiple regression models to make more accurate predictions. We can combine both best models on the grid search above to create a stacked model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "estimators = [\n",
    "    ('ElasticNet', ElasticNet(**params['ElasticNet'])),\n",
    "    ('DecisionTree', DecisionTreeRegressor(**params['DecisionTreeRegressor'])),\n",
    "]\n",
    "\n",
    "# Define the StackingRegressor\n",
    "StackingRegressorModel = StackingRegressor(estimators=estimators,\n",
    "                                           final_estimator=RandomForestRegressor(**params['RandomForestRegressor']))\n",
    "\n",
    "# Train the model on training data\n",
    "StackingRegressorModel.fit(X=Xtrain, y=ytrain)\n",
    "StackingRegressorScore = -cross_val_score(StackingRegressorModel, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Get the mean error\n",
    "meanError = 100*((10**(np.sqrt(np.mean(StackingRegressorScore))))-1)\n",
    "print(\"StackingRegressor Results: \")\n",
    "print(f\"Mean Test Score %: {meanError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woow! Looks like our model really improved by combining different models. Lets see if we can statistically prove that the difference is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing: is the model really better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val training Random Forest Model best model to get list of scores\n",
    "best_model.fit(X=Xtrain, y=ytrain)\n",
    "RandomForestScore = -cross_val_score(best_model, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Get the mean error\n",
    "meanError = 100*((10**(np.sqrt(np.mean(RandomForestScore))))-1)\n",
    "print(\"RandomForst Results: \")\n",
    "print(f\"Mean Test Score %: {meanError}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Performance scores for StackingRegressor and Elastic Net\n",
    "stacking_scores = StackingRegressorScore  # Replace with actual scores\n",
    "random_forest_scores = RandomForestScore  # Replace with actual scores\n",
    "\n",
    "# Calculate performance differences\n",
    "differences = [stacking - elastic_net for stacking, elastic_net in zip(stacking_scores, elastic_net_scores)]\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value = stats.ttest_rel(stacking_scores, random_forest_scores)\n",
    "\n",
    "alpha = 0.05  # Set your significance level\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in performance between Stacking and Random Forest.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in performance between the models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE DID IT!\n",
    "\n",
    "By combining models we created a new one that is significantly better than the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of StackingRegressor: 12.77%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fit and transform the stacked model                             \n",
    "StackingRegressorModel.fit_transform(X=Xtrain, y=ytrain)\n",
    "\n",
    "# Predict target values\n",
    "y_pred = StackingRegressorModel.predict(Xtest)\n",
    "\n",
    "# Get the mean error\n",
    "mean_error = mean_squared_error(ytest, y_pred)\n",
    "\n",
    "# Print mean error\n",
    "mean_relative_error = 100*((10**(np.sqrt(mean_error)))-1)\n",
    "print(f'Mean Absolute Error of StackingRegressor: {round(mean_relative_error, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on the whole dataset and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/victor/insper/6-sem/ml/ames_AndreVictor/models/stacking_model.pkl']"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "StackingRegressorModel.fit_transform(X=X, y=y)\n",
    "\n",
    "# Save the model and create the file if not exists\n",
    "model_path = pathlib.Path.cwd().parent / 'models' / 'stacking_model.pkl'\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ames",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbd4e51fa3f3bf2a2317a230e3ac8d5fc66f1a44e44c3383f6bf669b9d199507"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
